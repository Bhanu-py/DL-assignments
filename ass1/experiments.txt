The current model setup and results look quite good. The training accuracy is over 93% and your test accuracy is close to 90%, which the 85% threshold mentioned in the assignment. However, We can still experiment with a few things to possibly improve your model's performance:
- Learning Rate: You are currently using the Adam optimizer with its default learning rate of 0.001. You can try changing the learning rate to see if it affects your model's performance. However, be careful not to set the learning rate too high as it might cause the model to diverge, or too low as it might cause the model to learn too slowly.
- Batch Size: You are currently using a batch size of 128. You can try using a smaller or larger batch size. A smaller batch size might lead to slower training but could result in a better generalization. A larger batch size might lead to faster training but could result in a poorer generalization.
- Epochs: You are currently training your model for 20 epochs. You can try training your model for more epochs. However, be careful not to train for too many epochs as it might lead to overfitting.
- Dropout: You are currently using a dropout rate of 0.2. Dropout is a regularization technique that can help prevent overfitting. You can try using a higher dropout rate. However, be careful not to set the dropout rate too high as it might lead to underfitting.
- Additional Hidden Layers: You can try adding more hidden layers to your model. This could potentially increase the model's capacity to learn complex patterns in the data. However, be careful not to make your model too complex as it might lead to overfitting.
- Different Activation Functions: You are currently using the ReLU activation function in your hidden layers and the softmax activation function in your output layer. You can try using different activation functions to see if they improve your model's performance. However, the ReLU activation function is a good default choice for the hidden layers and the softmax activation function is a good choice for the output layer in a multiclass classification problem.


- First set batche size and then experiment on othe HPs(get the best from the github leaderboard)
